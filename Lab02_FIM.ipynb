{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Lab02: Frequent itemset mining\n",
    "\n",
    "- Student ID: \n",
    "- Student name: \n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space)\n",
    "\n",
    "    -------------------\n",
    "    return\n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    # TODO (hint: this function will be called in generateSearchSpace method.):\n",
    "    \n",
    "    return ret\n",
    "\n",
    "class TP:\n",
    "    def __init__(self, data=None, s=None, minSup=None):\n",
    "        self.data = data\n",
    "        self.s = {}\n",
    "\n",
    "        for key, support in sorted(s.items(), key=lambda item: item[1]):\n",
    "            self.s[key] = support\n",
    "        # TODO: why should we do this, answer it at the markdown below?\n",
    "\n",
    "        self.minSup = minSup\n",
    "        self.L = {}  # Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree = {}\n",
    "\n",
    "        search_space = {}\n",
    "        for item, support in self.s.items():\n",
    "            search_space[item] = {}\n",
    "\n",
    "            search_space[item]['itemset'] = [item]\n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    "\n",
    "            search_space[item]['pruned'] = False\n",
    "            # TODO:\n",
    "            # After finish implementing the algorithm tell me why should you use this\n",
    "            # instead of delete item directly from search_space and tree.\n",
    "\n",
    "            search_space[item]['support'] = support\n",
    "\n",
    "            tree[item] = {}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "\n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            '''\n",
    "\n",
    "        return tree, search_space\n",
    "\n",
    "    def computeItemsetSupport(self, itemset):\n",
    "\n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "       \n",
    "        return support\n",
    "\n",
    "    def get_sub_tree(self, k, tree, search_space, itter_node):\n",
    "        if k == 0:\n",
    "            return search_space[itter_node]['support']\n",
    "        subtree = search_space[itter_node]\n",
    "        for node in subtree.keys():\n",
    "            k-=1\n",
    "            self.get_sub_tree(k,tree,search_space,node)\n",
    "\n",
    "\n",
    "    def prune(self, k, tree, search_space):\n",
    "\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent\n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets.\n",
    "        '''\n",
    "        if self.L.get(k) is None: self.L[k] = []\n",
    "        # TODO\n",
    "\n",
    "\n",
    "\n",
    "    def generateSearchSpace(self, k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function)\n",
    "        '''\n",
    "        items = list(tree.keys())\n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l = len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l == 0: return  # Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            if search_space[a]['pruned']: continue\n",
    "\n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]\n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "\n",
    "                # TODO:\n",
    "                # First create newset using join set\n",
    "                \n",
    "                \n",
    "                # Second add newset to search_space\n",
    "                \n",
    "                \n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k + 1, sub_tree, sub_search_space)\n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree, search_space = self.initialize()  # generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "\n",
    "    def miningResults(self):\n",
    "        return self.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "data, s= readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnxbU77YRjaF",
    "outputId": "c3b158be-6b46-4a3c-9b71-6a92d3d31ded"
   },
   "outputs": [],
   "source": [
    "#\n",
    "a=TP(data=data,s=s, minSup=3000)\n",
    "print(a.miningResults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "**Why don't we compute support of items while reading data?**\n",
    "\n",
    "\n",
    "**why should we do sort**\n",
    "\n",
    "\n",
    "**study about python set and its advantages ?**\n",
    "\n",
    "\n",
    "**After finish implementing the algorithm tell me why should you use this? Instead of delete item directly from search_space and tree.**\n",
    "\n",
    "\n",
    "**Apriori algorithm and Tree Projection, tell me the differences of two algorithms.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). \n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: ntthuhang0131@gmail.com\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab01 - Frequent itemset mining.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
